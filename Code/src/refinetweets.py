# -*- coding: utf-8 -*-
"""refineTweets.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1YWpzhG0q-B94qQ2mAohUrTY9N9KtphsX
"""

pip install pandas-profiling

!pip install tweepy
!pip install wordcloud
!pip install textblob

import pandas_profiling
import tweepy                   
from google.colab import drive  
import json
import csv
from datetime import date
from datetime import datetime
import time
import numpy as np 
import pandas as pd 
import matplotlib.pyplot as plt 
import seaborn as sb
import re
import time
import string
import warnings

# for all NLP related operations on text
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import sent_tokenize, word_tokenize
from nltk.sentiment.vader import SentimentIntensityAnalyzer
from nltk.stem import WordNetLemmatizer
from nltk.stem.porter import *
from wordcloud import WordCloud
import matplotlib.animation as animation
import operator
import plotly.express as px
from collections import Counter

from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import f1_score, confusion_matrix
from sklearn.svm import SVC
from sklearn.naive_bayes import GaussianNB


# To consume Twitter's API
import tweepy
from tweepy import OAuthHandler 

# To identify the sentiment of text
from textblob import TextBlob

# ignoring all the warnings
warnings.filterwarnings("ignore", category=DeprecationWarning)

# downloading stopwords corpus
nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('vader_lexicon')
nltk.download('averaged_perceptron_tagger')
nltk.download('punkt')
stopwords = set(stopwords.words("english"))

drive.mount('/content/gdrive')
path = './gdrive/MyDrive/WDPS_data'

tweets_df = pd.read_json('/content/gdrive/MyDrive/WDPS_Data/ElectiontweetsFinal.json', lines=True)

len(tweets_df.index)

tweets_df.head(10)

tweets_df['content'].isna().sum()

tweets_df['id'].isna().sum()

#The tweets scraped provide a lot of information. For our project, we are only interested in the tweets.
#Create a new dataframe with tweets.
column = tweets_df[["renderedContent"]]
TweetText_df = column.copy()

TweetText_df.head(10)

#Determine Sentiment count of the dataframe.
def fetch_sentiment_using_textblob(text):
    analysis = TextBlob(text)
    # set sentiment 
    if analysis.sentiment.polarity >= 0:
        return 'positive'
    else: 
        return 'negative'

sentiments_using_textblob = TweetText_df.renderedContent.apply(lambda tweet: fetch_sentiment_using_textblob(tweet))
pd.DataFrame(sentiments_using_textblob.value_counts())

#Assign ID
TweetText_df['sentiment'] = sentiments_using_textblob
TweetText_df['id'] = TweetText_df.index + 1
TweetText_df.head()

def remove_pattern(text, pattern_regex):
    r = re.findall(pattern_regex, text)
    for i in r:
        text = re.sub(i, '', text)
    
    return text

# Create new column 'refinedTweets' to store cleaned up tweets.
TweetText_df['refinedTweets'] = np.vectorize(remove_pattern)(TweetText_df['renderedContent'], "@[\w]*: | *RT*")
TweetText_df.head(10)

# Remove @ and RT
TweetText_df['refinedTweets'] = np.vectorize(remove_pattern)(TweetText_df['renderedContent'], "@[\w]*: | *RT*")
TweetText_df.head(10)

#Remove hyperlinks contained in tweets.
cleanTweets = []

for index, row in TweetText_df.iterrows():
    words_without_links = [word for word in row.refinedTweets.split() if 'http' not in word]
    cleanTweets.append(' '.join(words_without_links))

TweetText_df['refinedTweets'] = cleanTweets
TweetText_df.head(10)

TweetText_df = TweetText_df[TweetText_df['refinedTweets']!='']
TweetText_df.head()

TweetText_df.drop_duplicates(subset=['refinedTweets'], keep=False)
TweetText_df.head()

TweetText_df = TweetText_df.reset_index(drop=True)
TweetText_df.head(10)

pip install unidecode

import unidecode

#Removing unnecessary characters using unidecode
TweetText_df['refinedTweets'] = TweetText_df['refinedTweets'].apply(lambda x : ' '.join([unidecode.unidecode(word) for word in x.split()]))
TweetText_df.head(10)

TweetText_df['refinedTweets'] = TweetText_df['refinedTweets'].str.replace("[^a-zA-Z# ]", "")
TweetText_df.head(10)

# Perform stopword removal.
# filter out all the stopwords.
# Create list of tuples containing stopwords and the sentimentType.
stopwords = set(stopwords)
cleanTweets = []

for index, row in TweetText_df.iterrows():
    
    words_without_stopwords = [word for word in row.refinedTweets.split() if not word in stopwords]
    cleanTweets.append(' '.join(words_without_stopwords))

TweetText_df['refinedTweets'] = cleanTweets
TweetText_df.head(10)

#Create a new column to find hashtags. Later on, we can use this column to count the number and determine the popularity of a hashtag.
TweetText_df['#'] = TweetText_df['refinedTweets'].apply(lambda x : ' '.join([word for word in x.split() if word.startswith('#')]))
TweetText_df.head(10)

frame = TweetText_df['#']

frame = pd.DataFrame(frame)

frame = frame.rename({'#':'Count(#)'},axis = 'columns')
frame[frame['Count(#)'] == ''] = 'No hashtags'
frame.head()

HTC_TweetText_df = pd.concat([TweetText_df,frame],axis = 1)

HTC_TweetText_df.drop('#',axis = 1,inplace = True)
HTC_TweetText_df.head()

#Perform Lemmitization.
lemmatizer = WordNetLemmatizer()
HTC_TweetText_df['refinedTweets'] = HTC_TweetText_df['refinedTweets'].apply(lambda x : ' '.join([lemmatizer.lemmatize(word) for word in x.split()]))

#Peform Stemming.
ps = PorterStemmer()
HTC_TweetText_df['refinedTweets'] = HTC_TweetText_df['refinedTweets'].apply(lambda x : ' '.join([ps.stem(word) for word in x.split()]))

from nltk.corpus import stopwords

len(HTC_TweetText_df)

#Perform Tokenization.
corpus = []
for i in range(0,79832):
    tweet = HTC_TweetText_df['refinedTweets'][i]
    tweet = tweet.lower()
    tweet = tweet.split()
    tweet = [ps.stem(word) for word in tweet if not word in set(stopwords.words('english'))]
    tweet = ' '.join(tweet)
    corpus.append(tweet)

#Collect hashtags based on their sentiments

hash_positive = []
hash_negative = []

def hashtag_extract(x):
    hashtags = []
    # Loop over the words in the tweet
    for i in x:
        ht = re.findall(r"#(\w+)", i)
        hashtags.append(ht)

    return hashtags

hash_positive = hashtag_extract(HTC_TweetText_df['refinedTweets'][HTC_TweetText_df['sentiment'] == 'positive'])

# extracting hashtags from racist/sexist tweets
hash_negative = hashtag_extract(HTC_TweetText_df['refinedTweets'][HTC_TweetText_df['sentiment'] == 'negative'])

# Converting a multidimensional list to a 1-D list
hash_positive = sum(hash_positive,[])
hash_negative = sum(hash_negative,[])

q = Counter(hash_positive)
q = dict(q.most_common())
l_positive_count = list(q.values())
l_positive_count[0:4]

r = Counter(hash_negative)
r = dict(r.most_common())
l_negative_count = list(r.values())
l_negative_count[0:4]

l_positive_values = list(q.keys())
l_positive_values[0:4]

l_negative_values = list(r.keys())
l_negative_values[0:4]

#Creating a dataframe to represent top 20 positive and negative hash words
l1 = pd.DataFrame(l_positive_values[0:20],columns = ['Positive_Words'])
l2 = pd.DataFrame(l_positive_count[0:20],columns = ['Positive_Count'])
l3 = pd.DataFrame(l_negative_values[0:20],columns = ['Negative_Words'])
l4 = pd.DataFrame(l_negative_count[0:20],columns = ['Negative_Count'])
z = pd.concat([l1,l2,l3,l4],axis = 1)
z

#Animated plot for positive words with their frequency
fig = px.bar(z, x="Positive_Words", y="Positive_Count",animation_frame="Positive_Count",
            hover_name="Positive_Words")
fig.layout.updatemenus[0].buttons[0].args[1]["frame"]["duration"] = 1200
fig.show()

# Animated plot for negative words with their frequency
fig = px.bar(z, x="Negative_Words", y="Negative_Count",animation_frame="Negative_Count",
            hover_name="Negative_Words")
fig.layout.updatemenus[0].buttons[0].args[1]["frame"]["duration"] = 1200
fig.show()

#Normal histogram of positive words
fig = px.bar(z, x="Positive_Words", y="Positive_Count",
            hover_name="Positive_Words",color = 'Positive_Count')
fig.show()

#Normal histogram of negative words
fig = px.bar(z, x="Negative_Words", y="Negative_Count",
            hover_name="Negative_Words",color= 'Negative_Count')
fig.show()

HTC_TweetText_df.head()

HTC_TweetText_df.to_csv('/content/gdrive/MyDrive/WDPS_Data/cleanedElectiontweetsFinal', index = False)

HTC_TweetText_df.to_json('/content/gdrive/MyDrive/WDPS_Data/cleanedElectiontweetsFinalJSON')

